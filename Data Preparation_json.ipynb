{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d68adc72-367c-496e-bcd3-50eb6f4e5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "PROJECT_ID = \"text Classification\" # Set this to your project name\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-imdb-email-dataset\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    raise ValueError(\"You must set a non-empty PROJECT_ID and make sure the project is created in GCP\") \n",
    "    \n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BUCKET_URI\"] = BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b547910-a03e-47f7-969b-3c9dc4ec764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "def maybe_download(url:str, download_dir:str=\"data\"):\n",
    "    \"\"\" Download the dataset from the web and extract the data \"\"\"\n",
    "    _, filename = os.path.split(url)\n",
    "    download_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(download_path):\n",
    "        print(\"Downloading data\")\n",
    "        response = requests.get(url)\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "        with open(download_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(\"File is already downloaded\")\n",
    "\n",
    "    tar = tarfile.open(download_path, \"r:gz\")\n",
    "    tar.extractall(os.path.join(download_dir,\"unzip\"))\n",
    "    tar.close()\n",
    "\n",
    "maybe_download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392e27d-ad76-48b3-85ff-9e151b7ada87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truncated_dataset(dataset_dir:str, n:int=2000):\n",
    "    \"\"\"It may take painstakenly long time to create a vertex AI dataset.\n",
    "    Therefore we're going to limit the data we'll be using\"\"\"\n",
    "\n",
    "    n_per_label = n//2\n",
    "    parent_dir, _ = os.path.split(dataset_dir)\n",
    "    truncated_dir = os.path.join(parent_dir, \"unzip_truncated\")\n",
    "    os.makedirs(truncated_dir, exist_ok=True)\n",
    "\n",
    "    for sub_dir in [\n",
    "        os.path.join(\"aclImdb\", \"train\", \"neg\"), \n",
    "        os.path.join(\"aclImdb\", \"train\", \"pos\"), \n",
    "        os.path.join(\"aclImdb\", \"test\", \"neg\"), \n",
    "        os.path.join(\"aclImdb\", \"test\", \"pos\")\n",
    "    ]:\n",
    "\n",
    "        os.makedirs(os.path.join(truncated_dir, sub_dir), exist_ok=True)\n",
    "        for f in os.listdir(os.path.join(dataset_dir, sub_dir)):\n",
    "            sample_id = int(f.split(\"_\")[0])\n",
    "            if sample_id < n_per_label:\n",
    "                shutil.copy(os.path.join(dataset_dir, sub_dir, f), os.path.join(truncated_dir, sub_dir, f))\n",
    "\n",
    "generate_truncated_dataset(os.path.join(\"data/unzip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d54b389-8db6-47dc-b691-ff6d28ef8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"textContent\": \"some review text\",\n",
      "    \"classificationAnnotation\": {\n",
      "        \"displayName\": \"positive\"\n",
      "    },\n",
      "    \"dataItemResourceLabels\": {\n",
      "        \"aiplatform.googleapis.com/ml_use\": \"training\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pydantic \n",
    "from typing import Dict,List, Tuple\n",
    "from typing_extensions import Literal\n",
    "\n",
    "# Format from https://cloud.google.com/vertex-ai/docs/text-data/classification/prepare-data\n",
    "class ClassificationAnnotation(pydantic.BaseModel):\n",
    "    displayName: Literal[\"positive\", \"negative\"]\n",
    "\n",
    "class DataItemResourceLabels(pydantic.BaseModel):\n",
    "    ml_use: Literal[\"training\", \"validation\", \"test\"] = pydantic.Field(alias=\"aiplatform.googleapis.com/ml_use\")\n",
    "    # Enables us to use ml_use=<x> instead of the long field name\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "class TextClassificationSample(pydantic.BaseModel):\n",
    "    textContent: str\n",
    "    classificationAnnotation: ClassificationAnnotation \n",
    "    dataItemResourceLabels: DataItemResourceLabels\n",
    "\n",
    "instance = TextClassificationSample(\n",
    "    textContent=\"some review text\", \n",
    "    classificationAnnotation=ClassificationAnnotation(displayName=\"positive\"),\n",
    "    dataItemResourceLabels=DataItemResourceLabels(ml_use=\"training\")\n",
    ")\n",
    "\n",
    "print(instance.json(by_alias=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e014feb4-89d6-4cc8-bd45-ee1ce48fac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from the GCS bucket\n",
      "\tFound 2001 train instances\n",
      "Reading test data from the GCS bucket\n",
      "\tFound 1013 validation instances and 987 test instances\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage \n",
    "random.seed(946021)\n",
    "\n",
    "\n",
    "def read_data(file_path:str) -> str:\n",
    "    \"\"\" Read a text file from a given path \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Solving the non-interchangeable valid content error during data import\n",
    "    data_processed = data.replace(\"\\u0085\", \" \").replace(\"\\u0096\", \" \")\n",
    "    return data_processed\n",
    "\n",
    "def generate_single_instance(file_path:str, ml_use:str) -> TextClassificationSample:\n",
    "    \"\"\" Given a filepath, create a single TextClassificationSample instance \"\"\"\n",
    "    label = None\n",
    "    if \"pos\" in file_path:\n",
    "        label = \"positive\"\n",
    "    elif \"neg\" in file_path:\n",
    "        label = \"negative\"\n",
    "    if label:\n",
    "        instance = TextClassificationSample(\n",
    "            textContent=read_data(file_path), \n",
    "            classificationAnnotation=ClassificationAnnotation(displayName=label),\n",
    "            dataItemResourceLabels=DataItemResourceLabels(ml_use=ml_use)\n",
    "        )\n",
    "        return instance\n",
    "    else:\n",
    "        raise ValueError(\"label cannot be None\")\n",
    "        \n",
    "def create_instances(data_dir: str) -> Tuple[List[TextClassificationSample], List[TextClassificationSample]]:\n",
    "\n",
    "    train_subdir = \"train\"\n",
    "    test_subdir = \"test\"\n",
    "    train_instances = []\n",
    "\n",
    "    print(f\"Reading training data from the GCS bucket\")\n",
    "    for root, _, files in os.walk(os.path.join(data_dir, train_subdir)):\n",
    "        for fname in files:\n",
    "            fpath = os.path.join(root, fname)\n",
    "            if fpath.endswith(\".txt\"):\n",
    "                instance = generate_single_instance(\n",
    "                    file_path=fpath, ml_use=\"training\"\n",
    "                )\n",
    "                train_instances.append(instance.json(by_alias=True, ensure_ascii=False)+'\\n')\n",
    "\n",
    "    print(f\"\\tFound {len(train_instances)} train instances\")\n",
    "\n",
    "    test_instances = []\n",
    "    valid_count, test_count = 0,0\n",
    "\n",
    "    print(f\"Reading test data from the GCS bucket\")\n",
    "    for root, _, files in os.walk(os.path.join(data_dir, test_subdir)):\n",
    "        for fname in files:\n",
    "            fpath = os.path.join(root, fname)\n",
    "            if fpath.endswith(\".txt\"):\n",
    "                if random.uniform(0,1.0)<0.5:\n",
    "                    valid_count += 1\n",
    "                    ml_use=\"validation\"\n",
    "                else:\n",
    "                    test_count += 1\n",
    "                    ml_use=\"test\"\n",
    "                \n",
    "                data = read_data(fpath)\n",
    "\n",
    "                instance = generate_single_instance(\n",
    "                    file_path=fpath, ml_use=ml_use\n",
    "                )\n",
    "                test_instances.append(instance.json(by_alias=True, ensure_ascii=False)+'\\n')\n",
    "\n",
    "    print(f\"\\tFound {valid_count} validation instances and {test_count} test instances\")\n",
    "\n",
    "    return train_instances, test_instances\n",
    "\n",
    "train_instances, test_instances = create_instances(os.path.join(\"data\", \"unzip_truncated\", \"aclImdb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c53b2829-655f-461e-a2a2-8bf1fc38e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(\"data\", \"train_instances.jsonl\"), \"w\") as f:\n",
    "    f.writelines(train_instances)\n",
    "\n",
    "with open(os.path.join(\"data\", \"test_instances.jsonl\"), \"w\") as f:\n",
    "    f.writelines(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bacf9bd-ba0e-4f22-8729-8d749546c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65ffd5a6-a1e6-445d-ae56-a67834cd5ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cust-eng-tac-tools-dev-imdb-email-dataset/test_instances.jsonl\n",
      "gs://cust-eng-tac-tools-dev-imdb-email-dataset/train_instances.jsonl\n",
      "Bucket gs://cust-eng-tac-tools-dev-imdb-email-dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if gsutil ls $BUCKET_URI; then\n",
    "    echo \"Bucket ${BUCKET_URI} already exists.\";\n",
    "else\n",
    "    echo \"Bucket ${BUCKET_URI} doesn't exist. Creating a new one\"\n",
    "    gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5ad38-df28-46b5-870a-a6ba561f9bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff32a1-df4b-4451-8d3e-2bd576ce9e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb20fca-ac60-41da-9a8a-875f0a0fd52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9634d-bdeb-4e45-a8b7-ac73d8957faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdb625-0806-43a4-ae81-eb44185d78c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e8571-9e01-40d1-af7b-8cfd1a8e916b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ecdd6-a600-40c8-80bf-27a2d01056d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa19fa6-cb6e-486c-85ff-e736a417705b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e190a2-5c72-4fa2-95a6-cf0ec2593916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238298c2-a4d4-4c91-b143-0a1ce718a48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e393b48-3fb4-4503-8e22-71d113a005c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "786b417a-df06-4c4d-bb04-63a33385a0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket gs://cust-eng-tac-tools-dev-imdb-email-dataset doesn't exist. Creating a new one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BucketNotFoundException: 404 gs://cust-eng-tac-tools-dev-imdb-email-dataset bucket does not exist.\n",
      "Creating gs://cust-eng-tac-tools-dev-imdb-email-dataset/...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if gsutil ls $BUCKET_URI; then\n",
    "    echo \"Bucket ${BUCKET_URI} already exists.\";\n",
    "else\n",
    "    echo \"Bucket ${BUCKET_URI} doesn't exist. Creating a new one\"\n",
    "    gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f998561-a787-44f3-90d5-ac3e9c2d6aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/train_instances.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  2.8 MiB/  2.8 MiB]                                                \n",
      "Operation completed over 1 objects/2.8 MiB.                                      \n",
      "Copying file://data/test_instances.jsonl [Content-Type=application/octet-stream]...\n",
      "- [1 files][  2.8 MiB/  2.8 MiB]                                                \n",
      "Operation completed over 1 objects/2.8 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data/train_instances.jsonl $BUCKET_URI\n",
    "!gsutil cp data/test_instances.jsonl $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6eb1ae-c2a2-4eaf-94c7-835e904344d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
